{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"./data\"\n",
    "example_submission_path = os.path.join(data_dir, \"gender_submission.csv\")\n",
    "train_csv_path = os.path.join(data_dir, \"train.csv\")\n",
    "test_csv_path = os.path.join(data_dir, \"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data description\n",
    "\n",
    "The data has been split into two groups:\n",
    "\n",
    "- training set (train.csv)\n",
    "- test set (test.csv)\n",
    "\n",
    "The training set should be used to build your machine learning models. For the training set, we provide the outcome (also known as the “ground truth”) for each passenger. Your model will be based on “features” like passengers’ gender and class. You can also use feature engineering to create new features.\n",
    "\n",
    "The test set should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.\n",
    "\n",
    "We also include gender_submission.csv, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.\n",
    "\n",
    "## Data Dictionary\n",
    "\n",
    "| Variable | Definition                                 | Key                                            |\n",
    "|----------|--------------------------------------------|------------------------------------------------|\n",
    "| survival | Survival                                   | 0 = No, 1 = Yes                                |\n",
    "| pclass   | Ticket class                               | 1 = 1st, 2 = 2nd, 3 = 3rd                      |\n",
    "| sex      | Sex                                        |                                                |\n",
    "| Age      | Age in years                               |                                                |\n",
    "| sibsp    | # of siblings / spouses aboard the Titanic |                                                |\n",
    "| parch    | # of parents / children aboard the Titanic |                                                |\n",
    "| ticket   | Ticket number                              |                                                |\n",
    "| fare     | Passenger fare                             |                                                |\n",
    "| cabin    | Cabin number                               |                                                |\n",
    "| embarked | Port of Embarkation                        | C = Cherbourg, Q = Queenstown, S = Southampton |\n",
    "\n",
    "\n",
    "## Variable Notes\n",
    "\n",
    "**pclass**: A proxy for socio-economic status (SES)\n",
    "- 1st = Upper\n",
    "- 2nd = Middle\n",
    "- 3rd = Lower\n",
    "\n",
    "**age**: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n",
    "\n",
    "**sibsp**: The dataset defines family relations in this way...\n",
    "- Sibling = brother, sister, stepbrother, stepsister\n",
    "- Spouse = husband, wife (mistresses and fiancés were ignored)\n",
    "\n",
    "**parch**: The dataset defines family relations in this way...\n",
    "- Parent = mother, father\n",
    "- Child = daughter, son, stepdaughter, stepson\n",
    "- Some children travelled only with a nanny, therefore parch=0 for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(train_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(test_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ticket(prefix, number):\n",
    "    return {'ticket_prefix': prefix, 'ticket_number': number}\n",
    "\n",
    "def transform_ticket_number(ticket_no, use_nan_map=False):\n",
    "    ticket_set = {\n",
    "        'LINE'\n",
    "    }\n",
    "    if ticket_no in ticket_set:\n",
    "        return pd.np.nan\n",
    "    return int(ticket_no)\n",
    "\n",
    "def transform_ticket_prefix(ticket_prefix):\n",
    "    if ticket_prefix == 'STON/O2.':\n",
    "        return 'STON/O 2.'\n",
    "    return ticket_prefix\n",
    "    \n",
    "def split_ticket(ticket):\n",
    "    parts = ticket.split()\n",
    "    if len(parts) == 0:\n",
    "        return make_ticket(pd.np.nan, np.pd.nan)\n",
    "    elif len(parts) == 1:\n",
    "        return make_ticket(pd.np.nan, transform_ticket_number(parts[0]))\n",
    "    return make_ticket(\n",
    "        transform_ticket_prefix(\" \".join(parts[0:-1])),\n",
    "        transform_ticket_number(parts[-1])\n",
    "    )\n",
    "\n",
    "def clean_cabin(cabin):\n",
    "    col_name = 'cabin_char'\n",
    "    if any([\n",
    "        cabin is None,\n",
    "        cabin == pd.np.nan,\n",
    "        isinstance(cabin, str) and len(cabin) == 0\n",
    "    ]):\n",
    "        return {col_name: pd.np.nan}\n",
    "    if isinstance(cabin, str):\n",
    "        return {col_name: cabin[0]}\n",
    "    return {col_name: pd.np.nan}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_df(original_df):\n",
    "    ticket_split_df = pd.DataFrame(original_df['Ticket'].apply(split_ticket).tolist())\n",
    "    cabin_df = pd.DataFrame(original_df['Cabin'].apply(clean_cabin).tolist())\n",
    "    \n",
    "    clean_df = prepare_feature_df(\n",
    "        original_df.join(ticket_split_df).join(cabin_df)\n",
    "    )\n",
    "    return clean_df\n",
    "\n",
    "def prepare_feature_df(clean_df):\n",
    "    return clean_df.drop(\n",
    "        columns=[\n",
    "            'Ticket',\n",
    "            'Name',\n",
    "            \n",
    "            # TODO add these back in later\n",
    "            \n",
    "            # Look at `set(prepared_df['ticket_prefix'])` to see ticket prefixes\n",
    "            # that might be similar\n",
    "            'ticket_prefix',\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def clean_train_df(df):\n",
    "    cleaned_df = clean_df(df)\n",
    "    X, y = split_train_x_y_df(cleaned_df)\n",
    "    return split_train_validate(X, y)\n",
    "    \n",
    "def split_train_x_y_df(cleaned_train_df):\n",
    "    y_col_name = 'Survived'\n",
    "    X = cleaned_train_df.drop(columns=[y_col_name])\n",
    "    y = cleaned_train_df[[y_col_name]]\n",
    "    return X, y\n",
    "\n",
    "def split_train_validate(X, y):\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "        X, y, test_size=0.33,\n",
    "    )\n",
    "    return X_train, X_valid, y_train, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = clean_train_df(train_df)\n",
    "pd.concat([X_train, X_valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_test_df = clean_df(test_df)\n",
    "cleaned_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def build_preprocessor():\n",
    "    numerical_cols = [\n",
    "        'PassengerId', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare'\n",
    "    ]\n",
    "    \n",
    "    categorical_cols = [\n",
    "        'Sex', 'Embarked',\n",
    "        'cabin_char', # TODO should this be converted to an ordered numerical value?\n",
    "    ]\n",
    "    \n",
    "    # Preprocessing for numerical data\n",
    "    numerical_transformer = SimpleImputer(strategy='constant')\n",
    "\n",
    "    # Preprocessing for categorical data\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "\n",
    "    # Bundle preprocessing for numerical and categorical data\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_transformer, numerical_cols),\n",
    "            ('cat', categorical_transformer, categorical_cols)\n",
    "        ])\n",
    "    return preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def define_model():\n",
    "    return LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "def build_pipeline():\n",
    "    # Bundle preprocessing and modeling code in a pipeline\n",
    "    my_pipeline = Pipeline(steps=[('preprocessor', build_preprocessor()),\n",
    "                                  ('model', define_model())\n",
    "                                 ])\n",
    "    return my_pipeline\n",
    "\n",
    "def evaluate_pipeline(pipeline, X_train, y_train, X_valid, y_valid):\n",
    "    # Preprocessing of training data, fit model \n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Preprocessing of validation data, get predictions\n",
    "    preds = pipeline.predict(X_valid)\n",
    "\n",
    "    # Evaluate the model\n",
    "    score = mean_absolute_error(y_valid, preds)\n",
    "    print('MAE:', score)\n",
    "    \n",
    "    pred_df = pd.DataFrame(\n",
    "        preds,\n",
    "        index=y_valid.index,\n",
    "        columns=['Survived']\n",
    "    )\n",
    "    \n",
    "    num_equal = len(y_valid[pred_df['Survived'] == y_valid['Survived']])\n",
    "    pct_equal = float(num_equal) / float(len(y_valid))\n",
    "    print('Percent correct pred:', pct_equal)\n",
    "    \n",
    "    return pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(train_df, test_df):\n",
    "    X_train, X_valid, y_train, y_valid = clean_train_df(train_df)\n",
    "    X_test = clean_df(test_df)\n",
    "    pipeline = build_pipeline()\n",
    "    validation_preds = evaluate_pipeline(pipeline, X_train, y_train, X_valid, y_valid)\n",
    "    \n",
    "    test_pred = pipeline.predict(X_test)\n",
    "    test_pred_df = pd.DataFrame(\n",
    "        test_pred,\n",
    "        index=X_test.index,\n",
    "        columns=['Survived']\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'validation': (X_valid, y_valid, validation_preds),\n",
    "        'test': (X_test, test_pred_df)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = main(train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(example_submission_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['test'][0].join(results['test'][1])[['PassengerId', 'Survived']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
